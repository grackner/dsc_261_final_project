{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grackner/dsc_261_final_project/blob/grackner/phi_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi Text Generation"
      ],
      "metadata": {
        "id": "I52_Ek1HQG8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct?library=transformers"
      ],
      "metadata": {
        "id": "UWzYJkccQ4n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH5kEhSCPpbf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True, dtype='float16')"
      ],
      "metadata": {
        "id": "uJTNWrbaPsbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "DsIe59dEZtJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "messages =[\n",
        "    {\"role\": \"user\", \"content\": \"Who is Michael Jordan?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=False)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "LSdKlKFDP5K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test dataset\n",
        "phi_df = pd.DataFrame(columns=['uuid', 'generated_article'])"
      ],
      "metadata": {
        "id": "d_9B5bi1c1J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_article():\n",
        "  messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Write the body of a news article, do not include a headline or title. It can be about any topic\"\n",
        "    },\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "          messages,\n",
        "          add_generation_prompt=True,\n",
        "          tokenize=True,\n",
        "          return_dict=True,\n",
        "          return_tensors=\"pt\",\n",
        "  ).to(model.device)\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens=100, use_cache=False, do_sample=True, temperature=0.9, top_p=0.95,top_k=50)\n",
        "  response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "  return response"
      ],
      "metadata": {
        "id": "FUYcCh6YlFSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate articles\n",
        "n = 100 # Number of datapoints to create\n",
        "for i in range(0, n + 1):\n",
        "  print(i)\n",
        "  # Add unique identifier for the row\n",
        "  phi_df.loc[i, 'uuid'] = str(uuid.uuid4())\n",
        "  response = generate_article()\n",
        "  phi_df.loc[i, 'generated_article'] = response"
      ],
      "metadata": {
        "id": "7KIbLXKPg239"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi_df.head()"
      ],
      "metadata": {
        "id": "-d3R9_4djedp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi_df.loc[1, 'generated_article']"
      ],
      "metadata": {
        "id": "vxfk8Xqnjgtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi_df.to_csv(\"phi_outputs.csv\")"
      ],
      "metadata": {
        "id": "ZRWEcGCLmY52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load dataset\n",
        "# path = kagglehub.dataset_download(\"gowrishankarp/newspaper-text-summarization-cnn-dailymail\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)\n",
        "\n",
        "# train_df = pd.read_csv(path + \"/cnn_dailymail/train.csv\")\n",
        "# test_df = pd.read_csv(path + \"/cnn_dailymail/test.csv\")\n",
        "# val_df = pd.read_csv(path + \"/cnn_dailymail/validation.csv\")"
      ],
      "metadata": {
        "id": "UHunOk8YQDa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loop through the train dataset\n",
        "# for index, row in train_df.head(1).iterrows():\n",
        "#   print(index)\n",
        "#   # Get article\n",
        "#   article = row[\"article\"]\n",
        "#   # Cut down to 100 words\n",
        "#   article = \" \".join(article.split()[:100])\n",
        "#   # Format the query\n",
        "#   messages = [\n",
        "#     {\n",
        "#         \"role\": \"user\",\n",
        "#         \"content\": [\n",
        "#             {\"type\": \"text\", \"text\": f\"Write a news article do not include a title\"}\n",
        "#             # {\"type\": \"text\", \"text\": f\"Write an article with a similar style to the following article example from CNN: {article}\"}\n",
        "#         ]\n",
        "#     },\n",
        "#   ]\n",
        "#   # Store query in df # TODO: Should happen in data cleaning?\n",
        "#   print(article)\n",
        "#   train_df.loc[index, 'query_article'] = messages[0]['content'][0]['text']\n",
        "#   ## Run query through inference\n",
        "#   inputs = processor.apply_chat_template(\n",
        "#     messages,\n",
        "#     add_generation_prompt=True,\n",
        "#     tokenize=True,\n",
        "#     return_dict=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#   ).to(model.device)\n",
        "\n",
        "#   outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "#   response = processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "#   # Store answer in df\n",
        "#   train_df.loc[index, 'model_output'] = response"
      ],
      "metadata": {
        "id": "9Sx2YT5eQgYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaVl5pXQo61d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}